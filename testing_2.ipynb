{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce207084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lightning AutoML - Complete Usage Examples\n",
    "Using sklearn built-in datasets\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import (\n",
    "    make_classification, \n",
    "    make_regression, \n",
    "    make_blobs,\n",
    "    load_iris,\n",
    "    load_diabetes,\n",
    "    load_wine\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Lightning AutoML\n",
    "from automl import LightningAutoML, auto_classification, auto_regression, auto_clustering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "420283bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 08:30:41 - LightningAutoML - INFO - Using device: cuda (NVIDIA GeForce RTX 3050 Laptop GPU)\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - GPU Memory: 4.00 GB\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - ============================================================\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - Lightning AutoML v1.2 - Memory Optimized\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - ML Type: SUPERVISED | Method: CLASSIFICATION\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - Device: cuda\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - Training samples: 105 | Test samples: 45\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - Features: 4\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - GPU Memory: 4095.5MB free / 4095.5MB total\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - ============================================================\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - Models to evaluate: 7\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - Time per model: ~25.7s\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - Trials per model: 30\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - CV folds: 5\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - \n",
      "============================================================\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - Optimizing LogisticRegression...\n",
      "2025-10-07 08:30:41 - LightningAutoML - INFO - GPU Memory: 0.0MB used / 4095.5MB total\n",
      "[I 2025-10-07 08:30:41,866] A new study created in memory with name: no-name-01ae09f2-5dc8-4aa1-acd7-f0bb59f99758\n",
      "2025-10-07 08:30:41 - lightning_ml.base_model - INFO - Initialized LogisticRegression on device: cuda\n",
      "2025-10-07 08:30:41 - lightning_ml.base_model - INFO - Neural model initialized: epochs=437, lr=0.07114476009343425, batch_size=16, optimizer=sgd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 1: Classification - Iris Dataset\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 08:30:46 - lightning_ml.base_model - INFO - Created optimizer: sgd\n",
      "2025-10-07 08:30:52 - LightningAutoML - ERROR - Error in evaluation: 'Sequential' object has no attribute 'weight'\n",
      "[I 2025-10-07 08:30:52,353] Trial 0 finished with value: -1.0 and parameters: {'epochs': 437, 'lr': 0.07114476009343425, 'batch_size': 16, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:30:52 - lightning_ml.base_model - INFO - Initialized LogisticRegression on device: cuda\n",
      "2025-10-07 08:30:52 - lightning_ml.base_model - INFO - Neural model initialized: epochs=737, lr=0.00011527987128232407, batch_size=16, optimizer=rmsprop\n",
      "2025-10-07 08:30:52 - lightning_ml.base_model - INFO - Created optimizer: rmsprop\n",
      "2025-10-07 08:31:12 - LightningAutoML - ERROR - Error in evaluation: 'Sequential' object has no attribute 'weight'\n",
      "[I 2025-10-07 08:31:12,975] Trial 1 finished with value: -1.0 and parameters: {'epochs': 737, 'lr': 0.00011527987128232407, 'batch_size': 16, 'optimizer': 'rmsprop'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:12 - lightning_ml.base_model - INFO - Initialized LogisticRegression on device: cuda\n",
      "2025-10-07 08:31:12 - lightning_ml.base_model - INFO - Neural model initialized: epochs=437, lr=0.07114476009343425, batch_size=16, optimizer=sgd\n",
      "2025-10-07 08:31:12 - lightning_ml.base_model - INFO - Created optimizer: sgd\n",
      "2025-10-07 08:31:25 - LightningAutoML - ERROR - ✗ Failed to train LogisticRegression: 'Sequential' object has no attribute 'weight'\n",
      "2025-10-07 08:31:25 - LightningAutoML - INFO - \n",
      "============================================================\n",
      "2025-10-07 08:31:25 - LightningAutoML - INFO - Optimizing LogisticRegressionNet...\n",
      "2025-10-07 08:31:25 - LightningAutoML - INFO - GPU Memory: 16.3MB used / 4095.5MB total\n",
      "[I 2025-10-07 08:31:25,994] A new study created in memory with name: no-name-42625e8a-992d-42fa-8c71-d1c48b581905\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,006] Trial 0 finished with value: -1.0 and parameters: {'epochs': 437, 'lr': 0.07114476009343425, 'batch_size': 16, 'dropout': 0.02904180608409973, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,015] Trial 1 finished with value: -1.0 and parameters: {'epochs': 118, 'lr': 0.0812324508558869, 'batch_size': 16, 'dropout': 0.15212112147976886, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - INFO - Adjusted batch size: 128 → 84 (free memory: 99.6%)\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,028] Trial 2 finished with value: -1.0 and parameters: {'epochs': 651, 'lr': 0.00026210878782654407, 'batch_size': 128, 'dropout': 0.09983689107917987, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,039] Trial 3 finished with value: -1.0 and parameters: {'epochs': 647, 'lr': 0.00032476735706274504, 'batch_size': 64, 'dropout': 0.15230688458668534, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,048] Trial 4 finished with value: -1.0 and parameters: {'epochs': 209, 'lr': 0.0030586566669785274, 'batch_size': 32, 'dropout': 0.15585553804470548, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,061] Trial 5 finished with value: -1.0 and parameters: {'epochs': 973, 'lr': 0.021154290797261225, 'batch_size': 16, 'dropout': 0.04424625102595975, 'optimizer': 'rmsprop'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,072] Trial 6 finished with value: -1.0 and parameters: {'epochs': 450, 'lr': 0.0006516990611177178, 'batch_size': 16, 'dropout': 0.07046211248738132, 'optimizer': 'rmsprop'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,084] Trial 7 finished with value: -1.0 and parameters: {'epochs': 795, 'lr': 0.00039459088111000007, 'batch_size': 32, 'dropout': 0.38563517334297287, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,097] Trial 8 finished with value: -1.0 and parameters: {'epochs': 877, 'lr': 0.0074112997810832455, 'batch_size': 16, 'dropout': 0.36480308916903204, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,110] Trial 9 finished with value: -1.0 and parameters: {'epochs': 207, 'lr': 0.013795402040204185, 'batch_size': 64, 'dropout': 0.26136641469099703, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - INFO - Adjusted batch size: 128 → 84 (free memory: 99.6%)\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,175] Trial 10 finished with value: -1.0 and parameters: {'epochs': 437, 'lr': 0.06289063790493159, 'batch_size': 128, 'dropout': 0.46614517095589375, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,233] Trial 11 finished with value: -1.0 and parameters: {'epochs': 134, 'lr': 0.0914857292426553, 'batch_size': 16, 'dropout': 0.010867956277513302, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,273] Trial 12 finished with value: -1.0 and parameters: {'epochs': 352, 'lr': 0.0367902118252243, 'batch_size': 16, 'dropout': 0.2171781358782056, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,312] Trial 13 finished with value: -1.0 and parameters: {'epochs': 316, 'lr': 0.0019113354590616638, 'batch_size': 16, 'dropout': 0.13349553647268483, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,350] Trial 14 finished with value: -1.0 and parameters: {'epochs': 591, 'lr': 0.09915049054117925, 'batch_size': 16, 'dropout': 0.23737724784809106, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,396] Trial 15 finished with value: -1.0 and parameters: {'epochs': 490, 'lr': 0.006577754999600273, 'batch_size': 16, 'dropout': 0.039231192483390354, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,436] Trial 16 finished with value: -1.0 and parameters: {'epochs': 102, 'lr': 0.03210255527416334, 'batch_size': 64, 'dropout': 0.1793703544422819, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - INFO - Adjusted batch size: 128 → 84 (free memory: 99.6%)\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,484] Trial 17 finished with value: -1.0 and parameters: {'epochs': 306, 'lr': 0.00010950172157239302, 'batch_size': 128, 'dropout': 0.3103263964810392, 'optimizer': 'rmsprop'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,523] Trial 18 finished with value: -1.0 and parameters: {'epochs': 213, 'lr': 0.010956271902968556, 'batch_size': 32, 'dropout': 0.10109430886725324, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,561] Trial 19 finished with value: -1.0 and parameters: {'epochs': 762, 'lr': 0.041356051276134155, 'batch_size': 16, 'dropout': 0.005494594621885673, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,599] Trial 20 finished with value: -1.0 and parameters: {'epochs': 376, 'lr': 0.0019613418944192614, 'batch_size': 16, 'dropout': 0.09576123471803676, 'optimizer': 'rmsprop'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - INFO - Adjusted batch size: 128 → 84 (free memory: 99.6%)\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,648] Trial 21 finished with value: -1.0 and parameters: {'epochs': 676, 'lr': 0.00010058617670929793, 'batch_size': 128, 'dropout': 0.10600687622555824, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - INFO - Adjusted batch size: 128 → 84 (free memory: 99.6%)\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,690] Trial 22 finished with value: -1.0 and parameters: {'epochs': 565, 'lr': 0.0010422162819651878, 'batch_size': 128, 'dropout': 0.1865947415646877, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - INFO - Adjusted batch size: 128 → 84 (free memory: 99.6%)\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,732] Trial 23 finished with value: -1.0 and parameters: {'epochs': 703, 'lr': 0.00023590970281014566, 'batch_size': 128, 'dropout': 0.05464985159577222, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - INFO - Adjusted batch size: 128 → 84 (free memory: 99.6%)\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,795] Trial 24 finished with value: -1.0 and parameters: {'epochs': 521, 'lr': 0.017438563822692585, 'batch_size': 128, 'dropout': 0.10707132787262676, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - INFO - Adjusted batch size: 128 → 84 (free memory: 99.6%)\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,850] Trial 25 finished with value: -1.0 and parameters: {'epochs': 588, 'lr': 0.005120458251208574, 'batch_size': 128, 'dropout': 0.2852429644788361, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,896] Trial 26 finished with value: -1.0 and parameters: {'epochs': 886, 'lr': 0.05120682156358568, 'batch_size': 32, 'dropout': 0.2004831188261247, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,939] Trial 27 finished with value: -1.0 and parameters: {'epochs': 421, 'lr': 0.0233552229966241, 'batch_size': 64, 'dropout': 0.13677573007437158, 'optimizer': 'adam'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:26 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:26,991] Trial 28 finished with value: -1.0 and parameters: {'epochs': 266, 'lr': 0.001007194880671227, 'batch_size': 16, 'dropout': 0.07486177403644836, 'optimizer': 'rmsprop'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:27 - LightningAutoML - ERROR - Error in evaluation: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "[I 2025-10-07 08:31:27,036] Trial 29 finished with value: -1.0 and parameters: {'epochs': 624, 'lr': 0.00020134193311447158, 'batch_size': 64, 'dropout': 0.16305196369422384, 'optimizer': 'sgd'}. Best is trial 0 with value: -1.0.\n",
      "2025-10-07 08:31:27 - LightningAutoML - ERROR - ✗ Failed to train LogisticRegressionNet: LogisticRegressionNet.__init__() missing 2 required positional arguments: 'n_features' and 'n_classes'\n",
      "2025-10-07 08:31:27 - LightningAutoML - INFO - \n",
      "============================================================\n",
      "2025-10-07 08:31:27 - LightningAutoML - INFO - Optimizing DecisionTree...\n",
      "2025-10-07 08:31:27 - LightningAutoML - INFO - GPU Memory: 16.3MB used / 4095.5MB total\n",
      "[I 2025-10-07 08:31:27,312] A new study created in memory with name: no-name-59497072-0034-4e83-9d8c-8c54c4eb6ee0\n",
      "2025-10-07 08:31:27 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:27 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=9, min_samples_split=20, min_samples_leaf=8\n",
      "2025-10-07 08:31:27 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "2025-10-07 08:31:28 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:28 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=9, min_samples_split=20, min_samples_leaf=8\n",
      "2025-10-07 08:31:28 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "2025-10-07 08:31:29 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:29 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=9, min_samples_split=20, min_samples_leaf=8\n",
      "2025-10-07 08:31:29 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "2025-10-07 08:31:29 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:29 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=9, min_samples_split=20, min_samples_leaf=8\n",
      "2025-10-07 08:31:29 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "2025-10-07 08:31:30 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:30 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=9, min_samples_split=20, min_samples_leaf=8\n",
      "2025-10-07 08:31:30 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "[I 2025-10-07 08:31:31,294] Trial 0 finished with value: 0.9047619047619048 and parameters: {'max_depth': 9, 'min_samples_split': 20, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.9047619047619048.\n",
      "2025-10-07 08:31:31 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:31 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=13, min_samples_split=4, min_samples_leaf=2\n",
      "2025-10-07 08:31:31 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "2025-10-07 08:31:32 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:32 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=13, min_samples_split=4, min_samples_leaf=2\n",
      "2025-10-07 08:31:32 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "2025-10-07 08:31:32 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:32 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=13, min_samples_split=4, min_samples_leaf=2\n",
      "2025-10-07 08:31:32 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "2025-10-07 08:31:33 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:33 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=13, min_samples_split=4, min_samples_leaf=2\n",
      "2025-10-07 08:31:33 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "2025-10-07 08:31:33 - lightning_ml.base_model - INFO - Initialized DecisionTreeClassifier on device: cuda\n",
      "2025-10-07 08:31:33 - lightning_ml.base_model - INFO - Tree model initialized: max_depth=13, min_samples_split=4, min_samples_leaf=2\n",
      "2025-10-07 08:31:33 - lightning_ml.base_model - INFO - Model fitted: n_samples=84, n_features=4\n",
      "[W 2025-10-07 08:31:34,060] Trial 1 failed with parameters: {'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 2} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Smile\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"d:\\Auto_ML\\new_auto_ml\\automl.py\", line 516, in objective\n",
      "    return self._evaluate_model(model_class, model_name, params)\n",
      "  File \"d:\\Auto_ML\\new_auto_ml\\automl.py\", line 378, in _evaluate_model\n",
      "    model.fit(X_tr_t, y_tr_t, verbose=False)\n",
      "  File \"d:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py\", line 920, in fit\n",
      "    self.tree_ = self._build_tree_with_counts(X, y)\n",
      "  File \"d:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py\", line 881, in _build_tree_with_counts\n",
      "    node = self._build_tree(X, y, depth)\n",
      "  File \"d:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py\", line 216, in _build_tree\n",
      "    node.right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
      "  File \"d:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py\", line 216, in _build_tree\n",
      "    node.right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
      "  File \"d:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py\", line 215, in _build_tree\n",
      "    node.left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
      "  File \"d:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py\", line 199, in _build_tree\n",
      "    best_feature, best_threshold, best_gain = self._find_best_split(\n",
      "  File \"d:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py\", line 153, in _find_best_split\n",
      "    left_impurity = self._calculate_impurity(y[left_mask])\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-07 08:31:34,062] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     18\u001b[0m automl_cls \u001b[38;5;241m=\u001b[39m LightningAutoML(\n\u001b[0;32m     19\u001b[0m     ml_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervised\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mautoml_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     34\u001b[0m predictions \u001b[38;5;241m=\u001b[39m automl_cls\u001b[38;5;241m.\u001b[39mpredict()\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\automl.py:526\u001b[0m, in \u001b[0;36mLightningAutoML.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    520\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    521\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    522\u001b[0m     sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    523\u001b[0m )\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 526\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_per_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;167;43;01mException\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m study\u001b[38;5;241m.\u001b[39mbest_trial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32mc:\\Users\\Smile\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Smile\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Smile\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Smile\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Smile\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\automl.py:516\u001b[0m, in \u001b[0;36mLightningAutoML.fit.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[0;32m    515\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hyperparameter_space(model_name, trial)\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\automl.py:378\u001b[0m, in \u001b[0;36mLightningAutoML._evaluate_model\u001b[1;34m(self, model_class, model_name, params)\u001b[0m\n\u001b[0;32m    375\u001b[0m X_val_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(X_val)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(current_device)\n\u001b[0;32m    376\u001b[0m y_val_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_val)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(current_device)\n\u001b[1;32m--> 378\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(X_val_t, y_val_t)\n\u001b[0;32m    380\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py:920\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, verbose)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding decision tree...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 920\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree_with_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# Initialize and calculate feature importances\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_importances_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_)\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py:881\u001b[0m, in \u001b[0;36mDecisionTreeClassifier._build_tree_with_counts\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_build_tree_with_counts\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor, y: torch\u001b[38;5;241m.\u001b[39mTensor, \n\u001b[0;32m    879\u001b[0m                             depth: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TreeNode:\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build tree and store class counts in leaf nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mis_leaf():\n\u001b[0;32m    884\u001b[0m         values, counts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(y, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py:216\u001b[0m, in \u001b[0;36mDecisionTreeBase._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    213\u001b[0m right_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mleft_mask\n\u001b[0;32m    215\u001b[0m node\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X[left_mask], y[left_mask], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 216\u001b[0m node\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py:216\u001b[0m, in \u001b[0;36mDecisionTreeBase._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    213\u001b[0m right_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mleft_mask\n\u001b[0;32m    215\u001b[0m node\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X[left_mask], y[left_mask], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 216\u001b[0m node\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py:215\u001b[0m, in \u001b[0;36mDecisionTreeBase._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    212\u001b[0m left_mask \u001b[38;5;241m=\u001b[39m X[:, best_feature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m best_threshold\n\u001b[0;32m    213\u001b[0m right_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mleft_mask\n\u001b[1;32m--> 215\u001b[0m node\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m node\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X[right_mask], y[right_mask], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py:199\u001b[0m, in \u001b[0;36mDecisionTreeBase._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Find best split\u001b[39;00m\n\u001b[0;32m    198\u001b[0m feature_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_features(n_features)\n\u001b[1;32m--> 199\u001b[0m best_feature, best_threshold, best_gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_best_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_indices\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# If no good split found, create leaf\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_feature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m best_gain \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Auto_ML\\new_auto_ml\\lightning_ml\\tree.py:153\u001b[0m, in \u001b[0;36mDecisionTreeBase._find_best_split\u001b[1;34m(self, X, y, feature_indices)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Calculate weighted impurity\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m left_impurity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_impurity\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m right_impurity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_impurity(y[right_mask])\n\u001b[0;32m    156\u001b[0m weighted_impurity \u001b[38;5;241m=\u001b[39m (n_left \u001b[38;5;241m*\u001b[39m left_impurity \u001b[38;5;241m+\u001b[39m n_right \u001b[38;5;241m*\u001b[39m right_impurity) \u001b[38;5;241m/\u001b[39m n_samples\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE 1: Classification with Iris Dataset\n",
    "# =============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE 1: Classification - Iris Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Create AutoML instance\n",
    "automl_cls = LightningAutoML(\n",
    "    ml_type=\"supervised\",\n",
    "    method=\"classification\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    time_budget=180,  # 3 minutes\n",
    "    n_trials=30,      # 30 trials per model\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train models\n",
    "automl_cls.fit()\n",
    "\n",
    "# Make predictions\n",
    "predictions = automl_cls.predict()\n",
    "print(f\"\\nSample predictions: {predictions[:10]}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "results = automl_cls.evaluate()\n",
    "\n",
    "# View leaderboard\n",
    "print(\"\\nModel Leaderboard:\")\n",
    "print(automl_cls.get_leaderboard())\n",
    "\n",
    "# Get best model parameters\n",
    "params = automl_cls.get_model_params()\n",
    "print(f\"\\nBest Model: {params['model_name']}\")\n",
    "print(f\"Hyperparameters: {params['hyperparameters']}\")\n",
    "\n",
    "# Save model\n",
    "automl_cls.save_model(\"iris_model\")\n",
    "\n",
    "# Clear GPU cache\n",
    "automl_cls.clear_gpu_cache()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 2: Regression with Diabetes Dataset\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 2: Regression - Diabetes Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Create AutoML instance\n",
    "automl_reg = LightningAutoML(\n",
    "    ml_type=\"supervised\",\n",
    "    method=\"regression\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    time_budget=180,\n",
    "    n_trials=30,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train models\n",
    "automl_reg.fit()\n",
    "\n",
    "# Make predictions\n",
    "predictions = automl_reg.predict()\n",
    "print(f\"\\nSample predictions: {predictions[:10]}\")\n",
    "print(f\"Actual values: {y_test[:10]}\")\n",
    "\n",
    "# Evaluate\n",
    "results = automl_reg.evaluate()\n",
    "\n",
    "# View leaderboard\n",
    "print(\"\\nModel Leaderboard:\")\n",
    "print(automl_reg.get_leaderboard())\n",
    "\n",
    "# Save model\n",
    "automl_reg.save_model(\"diabetes_model\")\n",
    "automl_reg.clear_gpu_cache()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 3: Clustering with Synthetic Data\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 3: Clustering - Synthetic Blob Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate blob data\n",
    "X, true_labels = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    centers=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data (no y needed for clustering)\n",
    "X_train, X_test = train_test_split(X, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create AutoML instance\n",
    "automl_cluster = LightningAutoML(\n",
    "    ml_type=\"unsupervised\",\n",
    "    method=\"cluster\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    time_budget=120,\n",
    "    n_trials=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train models\n",
    "automl_cluster.fit()\n",
    "\n",
    "# Make predictions (cluster assignments)\n",
    "cluster_labels = automl_cluster.predict()\n",
    "print(f\"\\nCluster assignments: {cluster_labels[:20]}\")\n",
    "print(f\"Unique clusters: {np.unique(cluster_labels)}\")\n",
    "\n",
    "# Evaluate\n",
    "results = automl_cluster.evaluate()\n",
    "\n",
    "# View leaderboard\n",
    "print(\"\\nModel Leaderboard:\")\n",
    "print(automl_cluster.get_leaderboard())\n",
    "\n",
    "# Save model\n",
    "automl_cluster.save_model(\"clustering_model\")\n",
    "automl_cluster.clear_gpu_cache()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 4: Quick Classification with Wine Dataset (convenience function)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 4: Quick Classification - Wine Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Use convenience function\n",
    "automl = auto_classification(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    time_budget=120,\n",
    "    n_trials=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "results = automl.evaluate()\n",
    "print(\"\\nFinal Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# View top 3 models\n",
    "leaderboard = automl.get_leaderboard()\n",
    "print(\"\\nTop 3 Models:\")\n",
    "print(leaderboard.head(3))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 5: Custom Prediction on New Data\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 5: Predict on New Custom Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create some new data (same features as wine dataset)\n",
    "new_data = np.array([\n",
    "    [13.2, 2.8, 2.4, 20, 104, 2.9, 2.8, 0.5, 1.8, 5.7, 1.0, 3.2, 1100],\n",
    "    [12.8, 3.1, 2.3, 21, 101, 2.7, 2.6, 0.4, 1.9, 5.5, 0.9, 3.1, 1050],\n",
    "])\n",
    "\n",
    "# Use the trained model to predict\n",
    "predictions = automl.predict(new_data)\n",
    "print(f\"Predictions for new data: {predictions}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 6: Large Classification Dataset\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 6: Large Synthetic Classification Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate large dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train with shorter time budget for demo\n",
    "automl_large = LightningAutoML(\n",
    "    ml_type=\"supervised\",\n",
    "    method=\"classification\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    time_budget=240,  # 4 minutes\n",
    "    n_trials=40,\n",
    "    cv_folds=3,  # Faster with fewer folds\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "automl_large.fit()\n",
    "results = automl_large.evaluate()\n",
    "\n",
    "print(\"\\nBest model info:\")\n",
    "params = automl_large.get_model_params()\n",
    "print(f\"  Model: {params['model_name']}\")\n",
    "print(f\"  Score: {params['score']:.4f} ± {params['cv_std']:.4f}\")\n",
    "print(f\"  Training time: {params['training_time']:.2f}s\")\n",
    "\n",
    "automl_large.clear_gpu_cache()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE 7: DataFrame Input\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 7: Using Pandas DataFrame\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(\n",
    "    load_iris().data,\n",
    "    columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    ")\n",
    "df['target'] = load_iris().target\n",
    "\n",
    "# Split\n",
    "train_df = df.sample(frac=0.7, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "X_train = train_df.drop('target', axis=1)\n",
    "y_train = train_df['target']\n",
    "X_test = test_df.drop('target', axis=1)\n",
    "y_test = test_df['target']\n",
    "\n",
    "# Train (AutoML handles DataFrame automatically)\n",
    "automl_df = LightningAutoML(\n",
    "    ml_type=\"supervised\",\n",
    "    method=\"classification\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    time_budget=60,\n",
    "    n_trials=15,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "automl_df.fit()\n",
    "results = automl_df.evaluate()\n",
    "\n",
    "print(\"\\nDataFrame example completed!\")\n",
    "print(f\"Test accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "automl_df.clear_gpu_cache()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXAMPLES COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f946423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
